{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 6 Face Recognition usando OpenCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV Face Recognizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV cuenta con 3 reconocedores faciales integrados. Estos se pueden utilizar de manera independiente en este código simplemente cambiando una línea de código. Estos son: \n",
    "\n",
    "1. EigenFaces Face Recognizer Recognizer - `cv2.face.EigenFaceRecognizer_create()`\n",
    "2. FisherFaces Face Recognizer Recognizer - `cv2.face.FisherFaceRecognizer_create()`\n",
    "3. Local Binary Patterns Histograms (LBPH) Face Recognizer - `cv2.face.LBPHFaceRecognizer_create()`\n",
    "\n",
    "En este caso se estará utilizando el reconocedor LBPH. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Binary Patterns Histograms (LBPH) Face Recognizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Una explicación detallada de LBPH puede ser encontrada en [face detection](https://www.superdatascience.com/opencv-face-detection/).\n",
    "\n",
    "\n",
    "Los reconocedores de Eigen y Fisher son afectados por la luz y esta es una condición que no se puede garantizar en situaciones de la vida real. El reconocedor usando LBPH es una mejora para superar esta desventaja. Su enfoque es utilizar descriptores locales en la imagen. LBPH trata de encontrar una estructura de la imagen y lo hace mediante la comparación de cada pixel con los de su vecindario. \n",
    "\n",
    "Se toma una ventana de 3x3 y se mueve a través de la imagen, en cada movimiento se compara el pixel central con los vecinos. Los vecinos con una intensidad menor o igual al del pixel central se marcan utilizando un 1 y los demás con un 0. Estos valores dentro de la ventana se leen en el sentido de las agujas del reloj lo que creará un patrón binario como 11100011 el cual es específico para esta zona de la imagen. Haciendo esto a través de toda la imagen se tendrá una lista de patrones locales binarios. \n",
    "\n",
    "\n",
    "**LBP Labeling**\n",
    "\n",
    "![LBP labeling](img/lbp-labeling.png)\n",
    "\n",
    "Con lo anterior se tiene la parte de los patrones binarios, para la creación del histograma, se convierte cada patron en un número binario (binario -->  digital) y entonces se realiza un histograma de todos los valores decimales.  \n",
    "\n",
    "**Sample Histogram**\n",
    "\n",
    "![LBP labeling](img/histogram.png)\n",
    "\n",
    "Con este enfoque estaremos creando un histograma para cada cara en la imagen. Por lo cual, cuando tenemos un dataset de entrenamiento con 100 caras tendremos 100 histogramas diferentes que se almacenaran para realizar el proceso de reconocimiento posteriormente. El algoritmo sabe que cara pertenece cada histograma. Durante la etapa de reconocimiento se pasará una imagen al reconocedor, el cual calculará el histograma de la cara detectada en la imagen y lo comparará con los histogramas que tiene almacenados, para devolver la categoría que mejor coincida con la imagen en evaluación. \n",
    "\n",
    "En esta imagen podemos ver como los LBPH no son afectados por los factores de iluminación\n",
    "\n",
    "**LBP Faces**\n",
    "\n",
    "![LBP faces](img/lbph-faces.jpg)\n",
    "\n",
    "**[source](http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de Reconocimiento Facial en OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de reconocimiento facial se puede dividir en 3 etapas:\n",
    "\n",
    "1. **Preparar los datos de entrenamiento:** En este paso se leerán las imágenes de entrenamiento para cada persona con sus etiquetas, se detectaran las caras en cada imagen y se asignan a una etiqueta o label entero.\n",
    "2. **Entrenar el reconocedor:** En este paso entrenaremos el reconocedor de caras de LBPH enviándole/mostrándole la información que se ha preparado en el paso 1. \n",
    "3. **Testing:** En esta etapa enviaremos algunas imágenes de prueba, para evaluar si la predicción se realiza de manera correcta. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import OpenCV module\n",
    "import cv2\n",
    "#import os module for reading training data directories and paths\n",
    "import os\n",
    "#import numpy to convert python lists to numpy arrays as \n",
    "#it is needed by OpenCV face recognizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre mayor cantidad de imágenes por sujeto, los resultados serán mejores ya que el reconocedor será capaz de aprender datos de la misma persona desde diferentes puntos de vista. En este caso nuestro dataset tiene 12 imágenes de cada sujeto, los cuales se encuentran en el folder `training-data`, este contiene en su interior un folder para cada sujeto que deseamos reconocer  cada folder tiene el formato `sLabel (e.g. s1, s2)` donde el número es la etiqueta entera asignada a acada sujeto. \n",
    "\n",
    "\n",
    "```\n",
    "training-data\n",
    "|-------------- s1\n",
    "|               |-- 1.jpg\n",
    "|               |-- ...\n",
    "|               |-- 12.jpg\n",
    "|-------------- s2\n",
    "|               |-- 1.jpg\n",
    "|               |-- ...\n",
    "|               |-- 12.jpg\n",
    "```\n",
    "\n",
    "El folder _`test-data`_ contiene las imágenes que serán utilizadas para evaluar nuestro reconocedor luego de ser entrenado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas en OpenCV deben ser de tipo entero, por lo cual se establece una forma de mapeado entre los números y los nombres de las personas. En nuestro caso no se utiliza el 0, por lo cual se deja vacío en la lista que contiene los nombres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is no label 0 in our training data so subject name for index/label 0 is empty\n",
    "subjects = [\"\", \"Ruben Blades\", \"Elvis Presley\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el reconocedor OpenCV necesita dos arreglos, uno con los rostros (histograma de patrones) de los sujetos de en el conjunto de entrenamiento y el segundo vector contiene, en el mismo orden, las etiquetas de cada rostro\n",
    "\n",
    "Por lo cual, si nuestro dataset contiene datos en esta forma:\n",
    "\n",
    "```\n",
    "PERSON-1    PERSON-2   \n",
    "\n",
    "img1        img1         \n",
    "img2        img2\n",
    "```\n",
    "\n",
    "Las listas producidas tendrán la siguiente estructura. \n",
    "\n",
    "```\n",
    "FACES                        LABELS\n",
    "\n",
    "person1_img1_face              1\n",
    "person1_img2_face              1\n",
    "person2_img1_face              2\n",
    "person2_img2_face              2\n",
    "```\n",
    "\n",
    "\n",
    "Esta preparación se puede resumir como: \n",
    "\n",
    "1. Procesar la carpeta de entrenamiento, de donde se obtendrán la cantidad de personas que estarán en el reconocedor.\n",
    "2. Para cada sujeto se debe extraer la etiqueta que se le asignará y almacenarla en formato de entero. \n",
    "3. Leer las imágenes para cada personas y detectar la cara en cada una de estas. \n",
    "4. Añadir cada cara a la lista de caras y su etiqueta correspondiente a la lista de etiquetas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to detect face using OpenCV\n",
    "def detect_face(img):\n",
    "    #convert the test image to gray image as opencv face detector expects gray images\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #load OpenCV face detector, I am using LBP which is fast\n",
    "    #there is also a more accurate but slow Haar classifier\n",
    "    face_cascade = cv2.CascadeClassifier('opencv-files/lbpcascade_frontalface.xml')\n",
    "\n",
    "    #let's detect multiscale (some images may be closer to camera than others) images\n",
    "    #result is a list of faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)\n",
    "    \n",
    "    #if no faces are detected then return original img\n",
    "    if (len(faces) == 0):\n",
    "        return None, None\n",
    "    \n",
    "    #under the assumption that there will be only one face,\n",
    "    #extract the face area\n",
    "    (x, y, w, h) = faces[0]\n",
    "    \n",
    "    #return only the face part of the image\n",
    "    return gray[y:y+w, x:x+h], faces[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El detector LBP necesita trabajar con imágenes en escala de grises, de igual manera se debe realizar como primer paso un recorte de la ubicación del rostro dentro de la imagen. En este fragmento de código se realiza la conversión de la imagen a escala de grises y se utilizan el `cv2.CascadeClassifier` mediante un modelo de detección de frontal de rostros, para realizar el recorte de la cara en la imagen. Este método devuelve el (x, y, width, height) de la zona en la cual se encuentra el rostro, para que pueda ser extraida utilizando OpenCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will read all persons' training images, detect face from each image\n",
    "#and will return two lists of exactly same size, one list \n",
    "# of faces and another list of labels for each face\n",
    "def prepare_training_data(data_folder_path):\n",
    "    \n",
    "    #------STEP-1--------\n",
    "    #get the directories (one directory for each subject) in data folder\n",
    "    dirs = os.listdir(data_folder_path)\n",
    "    \n",
    "    #list to hold all subject faces\n",
    "    faces = []\n",
    "    #list to hold labels for all subjects\n",
    "    labels = []\n",
    "    \n",
    "    #let's go through each directory and read images within it\n",
    "    for dir_name in dirs:\n",
    "        \n",
    "        #our subject directories start with letter 's' so\n",
    "        #ignore any non-relevant directories if any\n",
    "        if not dir_name.startswith(\"s\"):\n",
    "            continue\n",
    "            \n",
    "        #------STEP-2--------\n",
    "        #extract label number of subject from dir_name\n",
    "        #format of dir name = slabel\n",
    "        #, so removing letter 's' from dir_name will give us label\n",
    "        label = int(dir_name.replace(\"s\", \"\"))\n",
    "        \n",
    "        #build path of directory containin images for current subject subject\n",
    "        #sample path_dir_subject = \"training-data/s1\"\n",
    "        path_dir_subject = data_folder_path + \"/\" + dir_name\n",
    "        \n",
    "        #get the images names that are inside the given subject directory\n",
    "        subject_images_names = os.listdir(path_dir_subject)\n",
    "        \n",
    "        #------STEP-3--------\n",
    "        #go through each image name, read image, \n",
    "        #detect face and add face to list of faces\n",
    "        for image_name in subject_images_names:\n",
    "            \n",
    "            #ignore system files like .DS_Store\n",
    "            if image_name.startswith(\".\"):\n",
    "                continue\n",
    "            \n",
    "            #build image path\n",
    "            #sample image path = training-data/s1/1.pgm\n",
    "            image_path = path_dir_subject + \"/\" + image_name\n",
    "\n",
    "            #read image\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            #display an image window to show the image \n",
    "            cv2.imshow(\"Training on image...\", image)\n",
    "            cv2.waitKey(100)\n",
    "            \n",
    "            #detect face\n",
    "            face, rect = detect_face(image)\n",
    "            \n",
    "            #------STEP-4--------\n",
    "            #for the purpose of this tutorial\n",
    "            #we will ignore faces that are not detected\n",
    "            if face is not None:\n",
    "                #add face to list of faces\n",
    "                faces.append(face)\n",
    "                #add label for this face\n",
    "                labels.append(label)\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1) \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return faces, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función cumple la misión de preparar los datos de entrenamiento, recibiendo la ruta de la carpeta de entrenamiento y devolviendo las listas de caras y etiquetas de cada cara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data prepared\n",
      "Total faces:  22\n",
      "Total labels:  22\n"
     ]
    }
   ],
   "source": [
    "#let's first prepare our training data\n",
    "#data will be in two lists of same size\n",
    "#one list will contain all the faces\n",
    "#and other list will contain respective labels for each face\n",
    "print(\"Preparing data...\")\n",
    "faces, labels = prepare_training_data(\"training-data\")\n",
    "print(\"Data prepared\")\n",
    "\n",
    "#print total faces and labels\n",
    "print(\"Total faces: \", len(faces))\n",
    "print(\"Total labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Face Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our LBPH face recognizer \n",
    "face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "#or use EigenFaceRecognizer by replacing above line with \n",
    "#face_recognizer = cv2.face.EigenFaceRecognizer_create()\n",
    "\n",
    "#or use FisherFaceRecognizer by replacing above line with \n",
    "#face_recognizer = cv2.face.FisherFaceRecognizer_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso procedemos a instanciar el reconocedor y posteriormente se realiza el entrenamiento del mismo utilizando el método  `train(faces-vector, labels-vector)` el cual recibe la lista de caras y etiquetas del conjunto de entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train our face recognizer of our training faces\n",
    "face_recognizer.train(faces, np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to draw rectangle on image \n",
    "#according to given (x, y) coordinates and \n",
    "#given width and heigh\n",
    "def draw_rectangle(img, rect):\n",
    "    (x, y, w, h) = rect\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "#function to draw text on give image starting from\n",
    "#passed (x, y) coordinates. \n",
    "def draw_text(img, text, x, y):\n",
    "    cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones dibujarán un rectángulo sobre el rostro detectado y escribiran la etiqueta que ha definido el detector que corresponde con el rostro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function recognizes the person in image passed\n",
    "#and draws a rectangle around detected face with name of the \n",
    "#subject\n",
    "def predict(test_img):\n",
    "    #make a copy of the image as we don't want to chang original image\n",
    "    img = test_img.copy()\n",
    "    #detect face from the image\n",
    "    face, rect = detect_face(img)\n",
    "\n",
    "    #predict the image using our face recognizer \n",
    "    label= face_recognizer.predict(face)\n",
    "    print(label[1])  #valor de confidence, es una distancia entre más pequeño más cerca por lo tanto mejor\n",
    "    #get name of respective label returned by face recognizer\n",
    "    label_text = subjects[label[0]]\n",
    "    \n",
    "    #draw a rectangle around face detected\n",
    "    draw_rectangle(img, rect)\n",
    "    #draw name of predicted person\n",
    "    draw_text(img, label_text, rect[0], rect[1]-5)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso utilizamos el reconocedor entrenado para definir una etiqueta para un rostro en una imagen de prueba. Para esto se utiliza el método `predict(face)`, este retorna una tupla que contendrá el label(entero) al cual el reconocedor asignó la imagen y también un valor de confianza/probabilidad de dicho resultado. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting images...\n",
      "60.978268227083475\n",
      "26.689962927284107\n",
      "Prediction complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting images...\")\n",
    "\n",
    "#load test images\n",
    "test_img1 = cv2.imread(\"test-data/test0.jpg\")\n",
    "test_img2 = cv2.imread(\"test-data/test6.jpg\")\n",
    "\n",
    "###Si no detecta caras en la imagen dará un error\n",
    "\n",
    "#perform a prediction\n",
    "predicted_img1 = predict(test_img1)\n",
    "predicted_img2 = predict(test_img2)\n",
    "print(\"Prediction complete\")\n",
    "\n",
    "#display both images\n",
    "cv2.imshow(subjects[1], predicted_img1)\n",
    "cv2.imshow(subjects[2], predicted_img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "098cc3ff4b9ee01c70b5b09b122bba8f97c0fa39f6d5d8ad09fa234770520a78"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

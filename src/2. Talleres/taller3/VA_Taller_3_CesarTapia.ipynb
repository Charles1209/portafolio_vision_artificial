{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TALLER 3 - DETECIÓN DE ROSTRO CON IMAGENES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al cargar la imagen: becky.jpeg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import customtkinter as ctk\n",
    "from PIL import Image, ImageTk\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# Lista de imágenes y el índice actual\n",
    "image_paths = [\n",
    "    \"becky.jpeg\",   # Asegúrate de que las imágenes estén en el directorio correcto\n",
    "    \"descarga.jpeg\",\n",
    "    \"sarocha.jpeg\"\n",
    "]\n",
    "image_index = 0\n",
    "\n",
    "# Variable para almacenar la imagen cargada\n",
    "current_image = None\n",
    "\n",
    "# Cargar el modelo de MediaPipe\n",
    "with open(r\"C:\\Users\\charl\\Desktop\\VA_environment\\2. Talleres\\taller3\\Taller3Files\\face_landmarker_v2_with_blendshapes.task\", \"rb\") as model_file:\n",
    "    model_data = model_file.read()\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_buffer=model_data)\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=True,\n",
    "    num_faces=1\n",
    ")\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "# Función para dibujar los puntos faciales en la imagen\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    face_landmarks_list = detection_result.face_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    for face_landmarks in face_landmarks_list:\n",
    "        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        face_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "        ])\n",
    "\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "        )\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_contours_style()\n",
    "        )\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_iris_connections_style()\n",
    "        )\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "# Función para mostrar la imagen sin detección\n",
    "def mostrar_imagen():\n",
    "    global current_image\n",
    "    if 0 <= image_index < len(image_paths):\n",
    "        image_path = image_paths[image_index]\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error al cargar la imagen: {image_path}\")\n",
    "            return\n",
    "\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        current_image = image  # Almacena la imagen en BGR para la detección\n",
    "        img = Image.fromarray(image_rgb)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        video_frame.imgtk = imgtk\n",
    "        video_frame.configure(image=imgtk)\n",
    "\n",
    "# Función para avanzar a la siguiente imagen\n",
    "def siguiente_imagen():\n",
    "    global image_index\n",
    "    image_index += 1\n",
    "    if image_index >= len(image_paths):\n",
    "        image_index = 0  # Reinicia al principio después de la última imagen\n",
    "    mostrar_imagen()\n",
    "\n",
    "# Función para realizar la detección facial\n",
    "def detectar_rostro():\n",
    "    if current_image is None:\n",
    "        return  # No hacer nada si no hay una imagen cargada\n",
    "\n",
    "    # Convertir la imagen a RGB para el detector\n",
    "    image_rgb = cv2.cvtColor(current_image, cv2.COLOR_BGR2RGB)\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "    \n",
    "    # Realizar la detección\n",
    "    detection_result = detector.detect(image)\n",
    "\n",
    "    # Dibujar los puntos faciales en la imagen\n",
    "    annotated_image = draw_landmarks_on_image(image_rgb, detection_result)\n",
    "\n",
    "    # Mostrar la imagen anotada\n",
    "    img = Image.fromarray(annotated_image)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_frame.imgtk = imgtk\n",
    "    video_frame.configure(image=imgtk)\n",
    "\n",
    "# Crear la ventana principal\n",
    "ctk.set_appearance_mode(\"system\")  # Modo de apariencia de la interfaz\n",
    "window = ctk.CTk()\n",
    "window.title(\"Detección Facial con OpenCV y MediaPipe\")\n",
    "\n",
    "# Crear un marco de imagen en la ventana\n",
    "video_frame = ctk.CTkLabel(window)\n",
    "video_frame.pack()\n",
    "\n",
    "# Botones de la interfaz\n",
    "button_frame = ctk.CTkFrame(window)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "iniciar_button = ctk.CTkButton(button_frame, text=\"Iniciar\", command=mostrar_imagen)\n",
    "iniciar_button.grid(row=0, column=0, padx=5)\n",
    "\n",
    "siguiente_button = ctk.CTkButton(button_frame, text=\"Siguiente\", command=siguiente_imagen)\n",
    "siguiente_button.grid(row=0, column=1, padx=5)\n",
    "\n",
    "detectar_button = ctk.CTkButton(button_frame, text=\"Detectar\", command=detectar_rostro)\n",
    "detectar_button.grid(row=0, column=2, padx=5)\n",
    "\n",
    "finalizar_button = ctk.CTkButton(button_frame, text=\"Finalizar\", command=window.destroy)\n",
    "finalizar_button.grid(row=0, column=3, padx=5)\n",
    "\n",
    "# Ejecutar la ventana de Tkinter\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TALLER 3 - DETECIÓN DE MANOS CON IMAGENES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hand_landmarker.task'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m current_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Cargar el modelo de MediaPipe para detección de manos\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhand_landmarker.task\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m model_file:  \u001b[38;5;66;03m# Cambia al archivo de modelo de manos\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     model_data \u001b[38;5;241m=\u001b[39m model_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     25\u001b[0m base_options \u001b[38;5;241m=\u001b[39m python\u001b[38;5;241m.\u001b[39mBaseOptions(model_asset_buffer\u001b[38;5;241m=\u001b[39mmodel_data)\n",
      "File \u001b[1;32mc:\\Users\\charl\\Desktop\\VA_environment\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hand_landmarker.task'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import customtkinter as ctk\n",
    "from PIL import Image, ImageTk\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# Lista de imágenes y el índice actual\n",
    "image_paths = [\n",
    "    \"Manos 1.jpeg\",   # Asegúrate de que las imágenes estén en el directorio correcto\n",
    "    \"Manos 2.jpeg\",\n",
    "    \"Manos 3.1.jpeg\"\n",
    "]\n",
    "image_index = 0\n",
    "\n",
    "# Variable para almacenar la imagen cargada\n",
    "current_image = None\n",
    "\n",
    "# Cargar el modelo de MediaPipe para detección de manos\n",
    "with open(\"hand_landmarker.task\", \"rb\") as model_file:  # Cambia al archivo de modelo de manos\n",
    "    model_data = model_file.read()\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_buffer=model_data)\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    num_hands=2  # Número máximo de manos a detectar\n",
    ")\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# Función para dibujar los puntos de referencia de la mano en la imagen\n",
    "def draw_hand_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    for hand_landmarks in hand_landmarks_list:\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "        ])\n",
    "\n",
    "        # Dibuja los puntos clave y las conexiones de las manos\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=hand_landmarks_proto,\n",
    "            connections=mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "# Función para mostrar la imagen sin detección\n",
    "def mostrar_imagen():\n",
    "    global current_image\n",
    "    if 0 <= image_index < len(image_paths):\n",
    "        image_path = image_paths[image_index]\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error al cargar la imagen: {image_path}\")\n",
    "            return\n",
    "\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        current_image = image  # Almacena la imagen en BGR para la detección\n",
    "        img = Image.fromarray(image_rgb)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        video_frame.imgtk = imgtk\n",
    "        video_frame.configure(image=imgtk)\n",
    "\n",
    "# Función para avanzar a la siguiente imagen\n",
    "def siguiente_imagen():\n",
    "    global image_index\n",
    "    image_index += 1\n",
    "    if image_index >= len(image_paths):\n",
    "        image_index = 0  # Reinicia al principio después de la última imagen\n",
    "    mostrar_imagen()\n",
    "\n",
    "# Función para realizar la detección de manos\n",
    "def detectar_manos():\n",
    "    if current_image is None:\n",
    "        return  # No hacer nada si no hay una imagen cargada\n",
    "\n",
    "    # Convertir la imagen a RGB para el detector\n",
    "    image_rgb = cv2.cvtColor(current_image, cv2.COLOR_BGR2RGB)\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "    \n",
    "    # Realizar la detección\n",
    "    detection_result = detector.detect(image)\n",
    "\n",
    "    # Dibujar los puntos de referencia de la mano en la imagen\n",
    "    annotated_image = draw_hand_landmarks_on_image(image_rgb, detection_result)\n",
    "\n",
    "    # Mostrar la imagen anotada\n",
    "    img = Image.fromarray(annotated_image)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_frame.imgtk = imgtk\n",
    "    video_frame.configure(image=imgtk)\n",
    "\n",
    "# Crear la ventana principal\n",
    "ctk.set_appearance_mode(\"light\")  # Modo de apariencia de la interfaz\n",
    "window = ctk.CTk()\n",
    "window.title(\"Detección de Manos con OpenCV y MediaPipe\")\n",
    "\n",
    "# Crear un marco de imagen en la ventana\n",
    "video_frame = ctk.CTkLabel(window)\n",
    "video_frame.pack()\n",
    "\n",
    "# Botones de la interfaz\n",
    "button_frame = ctk.CTkFrame(window)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "iniciar_button = ctk.CTkButton(button_frame, text=\"Iniciar\", command=mostrar_imagen)\n",
    "iniciar_button.grid(row=0, column=0, padx=5)\n",
    "\n",
    "siguiente_button = ctk.CTkButton(button_frame, text=\"Siguiente\", command=siguiente_imagen)\n",
    "siguiente_button.grid(row=0, column=1, padx=5)\n",
    "\n",
    "detectar_button = ctk.CTkButton(button_frame, text=\"Detectar\", command=detectar_manos)\n",
    "detectar_button.grid(row=0, column=2, padx=5)\n",
    "\n",
    "finalizar_button = ctk.CTkButton(button_frame, text=\"Finalizar\", command=window.destroy)\n",
    "finalizar_button.grid(row=0, column=3, padx=5)\n",
    "\n",
    "# Ejecutar la ventana de Tkinter\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TALLER 3 - TIEMPO REAL DE ROSTRO Y MANOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import cv2\n",
    "import imutils\n",
    "import customtkinter\n",
    "import numpy as np\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "\n",
    "# Variables globales\n",
    "cap = None\n",
    "det = False\n",
    "\n",
    "# Funciones de la aplicación\n",
    "def iniciar():\n",
    "    global cap\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    visualizar()\n",
    "\n",
    "def visualizar():\n",
    "    global cap\n",
    "    global det\n",
    "    if cap is not None:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            if det:\n",
    "                frame = modificar(frame)\n",
    "            frame = imutils.resize(frame, width=640)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            im = Image.fromarray(frame)\n",
    "            img = customtkinter.CTkImage(light_image=im, size=(frame.shape[1], frame.shape[0]))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "            lblVideo.after(10, visualizar)\n",
    "        else:\n",
    "            lblVideo.image = \"\"\n",
    "            cap.release()\n",
    "\n",
    "def detectar():\n",
    "    global det\n",
    "    det = not det\n",
    "\n",
    "def finalizar():\n",
    "    global cap\n",
    "    global det\n",
    "    det = False\n",
    "    cap.release()  # Libera la cámara\n",
    "    root.quit()    # Detiene el bucle de eventos de Tkinter\n",
    "    root.destroy() # Cierra la ventana de la interfaz\n",
    "\n",
    "def modificar(frame):\n",
    "    proc_frame = detectorFaceAndHand(frame)\n",
    "    return proc_frame\n",
    "\n",
    "# Funciones de detección\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# Configuración de visualización\n",
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)\n",
    "\n",
    "def detectorFaceAndHand(img):\n",
    "    global detector, hands\n",
    "    height, width, _ = img.shape\n",
    "    annotated_image = img.copy()\n",
    "\n",
    "    # Detección de rostro\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "    detection_result = detector.detect(image)\n",
    "    annotated_image = visualize(annotated_image, detection_result)\n",
    "\n",
    "    # Detección de manos\n",
    "    results_hands = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "            # Dibujar los puntos de las manos\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                annotated_image, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp.solutions.drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "            # Obtener las coordenadas de la muñeca para determinar si es derecha o izquierda\n",
    "            wrist = hand_landmarks.landmark[mp.solutions.hands.HandLandmark.WRIST]\n",
    "            hand_x = wrist.x\n",
    "\n",
    "            # Determinar si la mano es izquierda o derecha\n",
    "            hand_label = \"Derecha\" if hand_x > 0.5 else \"Izquierda\"\n",
    "\n",
    "            # Calcular la posición para mostrar la etiqueta en la imagen\n",
    "            text_x = int(hand_x * width)\n",
    "            text_y = int(wrist.y * height) - 10  # Coloca la etiqueta encima de la muñeca\n",
    "\n",
    "            # Dibujar la etiqueta en la imagen\n",
    "            cv2.putText(annotated_image, hand_label,\n",
    "                        (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "def visualize(image, detection_result) -> np.ndarray:\n",
    "    \"\"\"Dibuja cuadros delimitadores y puntos clave en la imagen de entrada.\"\"\"\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    for detection in detection_result.detections:\n",
    "        # Dibujar el cuadro delimitador del rostro\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = (bbox.origin_x, bbox.origin_y)\n",
    "        end_point = (bbox.origin_x + bbox.width, bbox.origin_y + bbox.height)\n",
    "        cv2.rectangle(image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "        # Dibujar puntos clave\n",
    "        for keypoint in detection.keypoints:\n",
    "            keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y, width, height)\n",
    "            cv2.circle(image, keypoint_px, 2, (0, 255, 0), -1)\n",
    "\n",
    "    return image\n",
    "\n",
    "def _normalized_to_pixel_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    \"\"\"Convierte las coordenadas normalizadas a píxeles.\"\"\"\n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    return x_px, y_px\n",
    "\n",
    "# Configuración de detección\n",
    "model_file = open(r'C:\\Users\\charl\\Desktop\\VA_environment\\2. Talleres\\taller3\\Taller3Files\\blaze_face_short_range.tflite', \"rb\")\n",
    "model_data = model_file.read()\n",
    "model_file.close()\n",
    "base_options = python.BaseOptions(model_asset_buffer=model_data)\n",
    "options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "detector = vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "# Configuración de MediaPipe para manos\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# Interfaz de usuario\n",
    "root = customtkinter.CTk()\n",
    "\n",
    "btnIniciar = customtkinter.CTkButton(root, text=\"Iniciar\", width=45, command=iniciar)\n",
    "btnIniciar.grid(column=0, row=0, padx=5, pady=5)\n",
    "\n",
    "btnFinalizar = customtkinter.CTkButton(root, text=\"Finalizar\", width=45, command=finalizar)\n",
    "btnFinalizar.grid(column=1, row=0, padx=5, pady=5)\n",
    "\n",
    "btnMediaPipe = customtkinter.CTkButton(root, text=\"Detectar\", width=45, command=detectar)\n",
    "btnMediaPipe.grid(column=2, row=0, padx=5, pady=5)\n",
    "\n",
    "lblVideo = customtkinter.CTkLabel(root, text=\"\")\n",
    "lblVideo.grid(column=0, row=1, columnspan=3)\n",
    "\n",
    "root.resizable(width=False, height=False)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TALLER 3 -  TIEMPO REAL DE ROSTRO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import cv2\n",
    "import imutils\n",
    "import customtkinter\n",
    "import numpy as np\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# Variables globales\n",
    "cap = None\n",
    "det = False\n",
    "\n",
    "# Funciones de la aplicación\n",
    "def iniciar():\n",
    "    global cap\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    visualizar()\n",
    "\n",
    "def visualizar():\n",
    "    global cap\n",
    "    global det\n",
    "    if cap is not None:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            if det:\n",
    "                frame = modificar(frame)\n",
    "            frame = imutils.resize(frame, width=640)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            im = Image.fromarray(frame)\n",
    "            img = customtkinter.CTkImage(light_image=im, size=(frame.shape[1], frame.shape[0]))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "            lblVideo.after(10, visualizar)\n",
    "        else:\n",
    "            lblVideo.image = \"\"\n",
    "            cap.release()\n",
    "\n",
    "def detectar():\n",
    "    global det\n",
    "    det = not det\n",
    "\n",
    "def finalizar():\n",
    "    global cap\n",
    "    global det\n",
    "    det = False\n",
    "    cap.release()  # Libera la cámara\n",
    "    root.quit()    # Detiene el bucle de eventos de Tkinter\n",
    "    root.destroy() # Cierra la ventana de la interfaz\n",
    "\n",
    "def modificar(frame):\n",
    "    proc_frame = detectorFace(frame)\n",
    "    return proc_frame\n",
    "\n",
    "# Función de detección de rostro\n",
    "def detectorFace(img):\n",
    "    height, width, _ = img.shape\n",
    "    annotated_image = img.copy()\n",
    "\n",
    "    # Detección de rostro\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "    detection_result = detector.detect(image)\n",
    "    annotated_image = visualize(annotated_image, detection_result)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "def visualize(image, detection_result) -> np.ndarray:\n",
    "    \"\"\"Dibuja cuadros delimitadores y puntos clave en la imagen de entrada.\"\"\"\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    for detection in detection_result.detections:\n",
    "        # Dibujar el cuadro delimitador del rostro\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = (bbox.origin_x, bbox.origin_y)\n",
    "        end_point = (bbox.origin_x + bbox.width, bbox.origin_y + bbox.height)\n",
    "        cv2.rectangle(image, start_point, end_point, (255, 0, 0), 3)\n",
    "\n",
    "        # Dibujar puntos clave\n",
    "        for keypoint in detection.keypoints:\n",
    "            keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y, width, height)\n",
    "            cv2.circle(image, keypoint_px, 2, (0, 255, 0), -1)\n",
    "\n",
    "    return image\n",
    "\n",
    "def _normalized_to_pixel_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    \"\"\"Convierte las coordenadas normalizadas a píxeles.\"\"\"\n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    return x_px, y_px\n",
    "\n",
    "# Configuración de detección de rostro\n",
    "model_file = open(r'C:\\Users\\charl\\Desktop\\VA_environment\\2. Talleres\\taller3\\Taller3Files\\blaze_face_short_range.tflite', \"rb\")\n",
    "model_data = model_file.read()\n",
    "model_file.close()\n",
    "base_options = python.BaseOptions(model_asset_buffer=model_data)\n",
    "options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "detector = vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "# Interfaz de usuario\n",
    "root = customtkinter.CTk()\n",
    "\n",
    "btnIniciar = customtkinter.CTkButton(root, text=\"Iniciar\", width=45, command=iniciar)\n",
    "btnIniciar.grid(column=0, row=0, padx=5, pady=5)\n",
    "\n",
    "btnFinalizar = customtkinter.CTkButton(root, text=\"Finalizar\", width=45, command=finalizar)\n",
    "btnFinalizar.grid(column=1, row=0, padx=5, pady=5)\n",
    "\n",
    "btnMediaPipe = customtkinter.CTkButton(root, text=\"Detectar\", width=45, command=detectar)\n",
    "btnMediaPipe.grid(column=2, row=0, padx=5, pady=5)\n",
    "\n",
    "lblVideo = customtkinter.CTkLabel(root, text=\"\")\n",
    "lblVideo.grid(column=0, row=1, columnspan=3)\n",
    "\n",
    "root.resizable(width=False, height=False)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TALLER 3 - TIEMPO REAL DE MANOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import cv2\n",
    "import imutils\n",
    "import customtkinter\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Variables globales\n",
    "cap = None\n",
    "det = False\n",
    "\n",
    "# Funciones de la aplicación\n",
    "def iniciar():\n",
    "    global cap\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    visualizar()\n",
    "\n",
    "def visualizar():\n",
    "    global cap\n",
    "    global det\n",
    "    if cap is not None:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            if det:\n",
    "                frame = modificar(frame)\n",
    "            frame = imutils.resize(frame, width=640)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            im = Image.fromarray(frame)\n",
    "            img = customtkinter.CTkImage(light_image=im, size=(frame.shape[1], frame.shape[0]))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "            lblVideo.after(10, visualizar)\n",
    "        else:\n",
    "            lblVideo.image = \"\"\n",
    "            cap.release()\n",
    "\n",
    "def detectar():\n",
    "    global det\n",
    "    det = not det\n",
    "\n",
    "def finalizar():\n",
    "    global cap\n",
    "    global det\n",
    "    det = False\n",
    "    cap.release()  # Libera la cámara\n",
    "    root.quit()    # Detiene el bucle de eventos de Tkinter\n",
    "    root.destroy() # Cierra la ventana de la interfaz\n",
    "\n",
    "def modificar(frame):\n",
    "    proc_frame = detectorHand(frame)\n",
    "    return proc_frame\n",
    "\n",
    "# Función de detección de manos\n",
    "def detectorHand(img):\n",
    "    height, width, _ = img.shape\n",
    "    annotated_image = img.copy()\n",
    "\n",
    "    # Detección de manos\n",
    "    results_hands = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "            # Dibujar los puntos de las manos\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                annotated_image, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp.solutions.drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "            # Obtener las coordenadas de la muñeca para determinar si es derecha o izquierda\n",
    "            wrist = hand_landmarks.landmark[mp.solutions.hands.HandLandmark.WRIST]\n",
    "            hand_x = wrist.x\n",
    "\n",
    "            # Determinar si la mano es izquierda o derecha\n",
    "            hand_label = \"Derecha\" if hand_x > 0.5 else \"Izquierda\"\n",
    "\n",
    "            # Calcular la posición para mostrar la etiqueta en la imagen\n",
    "            text_x = int(hand_x * width)\n",
    "            text_y = int(wrist.y * height) - 10  # Coloca la etiqueta encima de la muñeca\n",
    "\n",
    "            # Dibujar la etiqueta en la imagen\n",
    "            cv2.putText(annotated_image, hand_label,\n",
    "                        (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "# Configuración de MediaPipe para manos\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# Interfaz de usuario\n",
    "root = customtkinter.CTk()\n",
    "\n",
    "btnIniciar = customtkinter.CTkButton(root, text=\"Iniciar\", width=45, command=iniciar)\n",
    "btnIniciar.grid(column=0, row=0, padx=5, pady=5)\n",
    "\n",
    "btnFinalizar = customtkinter.CTkButton(root, text=\"Finalizar\", width=45, command=finalizar)\n",
    "btnFinalizar.grid(column=1, row=0, padx=5, pady=5)\n",
    "\n",
    "btnMediaPipe = customtkinter.CTkButton(root, text=\"Detectar\", width=45, command=detectar)\n",
    "btnMediaPipe.grid(column=2, row=0, padx=5, pady=5)\n",
    "\n",
    "lblVideo = customtkinter.CTkLabel(root, text=\"\")\n",
    "lblVideo.grid(column=0, row=1, columnspan=3)\n",
    "\n",
    "root.resizable(width=False, height=False)\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VA_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
